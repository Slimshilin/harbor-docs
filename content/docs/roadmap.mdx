---
title: Roadmap
description: The future of Harbor
---

Here's what we're working on and where Harbor is headed.

## Benchmarks

- [ ] **Adapt all compatible benchmarks into the Harbor registry** — Make it easy to run any major benchmark through Harbor with a single command.
- [ ] **Build new benchmarks** — Develop next-generation benchmarks like Terminal-Bench 3.0.
- [ ] **Help researchers and companies build & share benchmarks** — Provide tools and documentation to make benchmark creation accessible to everyone.
- [ ] **Multi-turn conversation evaluation** — Support for evaluating agents across extended, multi-turn interactions.
- [ ] **Task creation tooling** — Better tools for authoring and validating new tasks.

## Integrations

- [ ] **Integrate with Tinker for training** — Enable seamless workflows between Harbor evaluation and Tinker training infrastructure.
- [x] **Support LLM-as-a-judge** — Add first-class support for using language models as evaluation judges.

## Infrastructure

- [ ] **Improve visualization and analysis tooling** — Better dashboards, reports, and insights from your evaluation runs.
- [ ] **Build a hosted storage layer** — Persistent, shareable storage for trajectories, results, and artifacts.
- [ ] **Build hosted rollout infra** — Managed infrastructure for running evaluations at scale.
